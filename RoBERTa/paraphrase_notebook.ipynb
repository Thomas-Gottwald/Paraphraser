{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Example for using the Paraphraser"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paraphrase_pipeline as ppipe\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines import FillMaskPipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "unmasker = FillMaskPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "# unmasker = pipeline('fill-mask', model='roberta-large')\n",
    "paraphraser = ppipe.ParaphrasePipeline(unmasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The alkali metals are more similar to each other than the elements in any other group are to each other. For instance, when moving down the table, all known alkali metals show increasing atomic radius, decreasing electronegativity, increasing reactivity, and decreasing melting and boiling points as well as heats of fusion and vaporisation. In general, their densities increase when moving down the table, with the exception that potassium is less dense than sodium.'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "filename = r\"./../data/wikipedia/og/666-ORIG-31.txt\"\n",
    "with open(filename) as file:\n",
    "    originalText = file.read()\n",
    "originalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spun_text, df = paraphraser.parapherase(originalText, mask=0.1, range_replace=(1, 4), mark_replace=True, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The alkali metals are more similar to each other than the elements in any[ metal] group are to each other. For[ instance][,] when moving down the table, all known alkali[ groups] show increasing atomic radius,[ increasing] electronegativity, increasing reactivity, and[ increasing] melting and boiling points as well as heats of fusion and[ oxid]isation. In general, their densities increase when moving down the[ scale], with the exception that[ magnesium] is less dense than sodium.'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "spun_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               token_str     score    state\n",
       "index token                                \n",
       "85    35797   Ġpotassium  0.436210  ignored\n",
       "      16904     Ġlithium  0.195053   looked\n",
       "      18051   Ġaluminium  0.068147   looked\n",
       "      36165   Ġmagnesium  0.051567   chosen\n",
       "65    31973         Ġion  0.411731  ignored\n",
       "      34774    Ġcrystall  0.235001   looked\n",
       "      29406       Ġvapor  0.215348   looked\n",
       "      44322        Ġoxid  0.025223   chosen\n",
       "79    2103        Ġtable  0.964238  ignored\n",
       "      889          Ġlist  0.007279   looked\n",
       "      3907         Ġtree  0.003146   looked\n",
       "      3189        Ġscale  0.002226   chosen\n",
       "16    97          Ġother  0.920231  ignored\n",
       "      4204        Ġmetal  0.014210   chosen\n",
       "      4747     Ġchemical  0.006795   looked\n",
       "      12628  Ġfunctional  0.005189   looked\n",
       "53    20910  Ġdecreasing  0.526035  ignored\n",
       "      2284   Ġincreasing  0.334759   chosen\n",
       "      795         Ġlower  0.016758   looked\n",
       "      723        Ġhigher  0.016246   looked\n",
       "36    10340      Ġmetals  0.974674  ignored\n",
       "      4785     Ġelements  0.016047   looked\n",
       "      4204        Ġmetal  0.001942   looked\n",
       "      1134       Ġgroups  0.001286   chosen\n",
       "24    1246      Ġexample  0.854074  ignored\n",
       "      4327     Ġinstance  0.140817   chosen\n",
       "      6676   Ġcomparison  0.001134   looked\n",
       "      9770     Ġstarters  0.000450   looked\n",
       "42    20910  Ġdecreasing  0.627233  ignored\n",
       "      2284   Ġincreasing  0.311633   chosen\n",
       "      1130    Ġincreased  0.010608   looked\n",
       "      8065    Ġdecreased  0.006566   looked\n",
       "25    6                ,  0.999001  ignored\n",
       "      35               :  0.000813   looked\n",
       "      131              ;  0.000060   looked\n",
       "      2156            Ġ,  0.000023   chosen"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>token_str</th>\n      <th>score</th>\n      <th>state</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th>token</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">85</th>\n      <th>35797</th>\n      <td>Ġpotassium</td>\n      <td>0.436210</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>16904</th>\n      <td>Ġlithium</td>\n      <td>0.195053</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>18051</th>\n      <td>Ġaluminium</td>\n      <td>0.068147</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>36165</th>\n      <td>Ġmagnesium</td>\n      <td>0.051567</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">65</th>\n      <th>31973</th>\n      <td>Ġion</td>\n      <td>0.411731</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>34774</th>\n      <td>Ġcrystall</td>\n      <td>0.235001</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>29406</th>\n      <td>Ġvapor</td>\n      <td>0.215348</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>44322</th>\n      <td>Ġoxid</td>\n      <td>0.025223</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">79</th>\n      <th>2103</th>\n      <td>Ġtable</td>\n      <td>0.964238</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>Ġlist</td>\n      <td>0.007279</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>3907</th>\n      <td>Ġtree</td>\n      <td>0.003146</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>3189</th>\n      <td>Ġscale</td>\n      <td>0.002226</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">16</th>\n      <th>97</th>\n      <td>Ġother</td>\n      <td>0.920231</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>4204</th>\n      <td>Ġmetal</td>\n      <td>0.014210</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>4747</th>\n      <td>Ġchemical</td>\n      <td>0.006795</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>12628</th>\n      <td>Ġfunctional</td>\n      <td>0.005189</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">53</th>\n      <th>20910</th>\n      <td>Ġdecreasing</td>\n      <td>0.526035</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>2284</th>\n      <td>Ġincreasing</td>\n      <td>0.334759</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>Ġlower</td>\n      <td>0.016758</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>723</th>\n      <td>Ġhigher</td>\n      <td>0.016246</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">36</th>\n      <th>10340</th>\n      <td>Ġmetals</td>\n      <td>0.974674</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>4785</th>\n      <td>Ġelements</td>\n      <td>0.016047</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>4204</th>\n      <td>Ġmetal</td>\n      <td>0.001942</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>1134</th>\n      <td>Ġgroups</td>\n      <td>0.001286</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">24</th>\n      <th>1246</th>\n      <td>Ġexample</td>\n      <td>0.854074</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>4327</th>\n      <td>Ġinstance</td>\n      <td>0.140817</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>6676</th>\n      <td>Ġcomparison</td>\n      <td>0.001134</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>9770</th>\n      <td>Ġstarters</td>\n      <td>0.000450</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">42</th>\n      <th>20910</th>\n      <td>Ġdecreasing</td>\n      <td>0.627233</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>2284</th>\n      <td>Ġincreasing</td>\n      <td>0.311633</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>1130</th>\n      <td>Ġincreased</td>\n      <td>0.010608</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>8065</th>\n      <td>Ġdecreased</td>\n      <td>0.006566</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">25</th>\n      <th>6</th>\n      <td>,</td>\n      <td>0.999001</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>:</td>\n      <td>0.000813</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>;</td>\n      <td>0.000060</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>2156</th>\n      <td>Ġ,</td>\n      <td>0.000023</td>\n      <td>chosen</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ayn Rand (; born Alisa Zinovyevna Rosenbaum; \\xa0– March 6, 1982) was a Russian-American writer and philosopher. She is known for her two best-selling novels, \"The Fountainhead\" and \"Atlas Shrugged\", and for developing a philosophical system she named Objectivism. Educated in Russia, she moved to the United States in 1926. She had a play produced on Broadway in 1935 and 1936. After two early novels that were initially unsuccessful, she achieved fame with her 1943 novel, \"The Fountainhead\". In 1957, Rand published her best-known work, the novel \"Atlas Shrugged\". Afterward, she turned to non-fiction to promote her philosophy, publishing her own periodicals and releasing several collections of essays until her death in 1982.'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "filename = r\"./../data/wikipedia/og/339-ORIG-2.txt\"\n",
    "with open(filename) as file:\n",
    "    originalText = file.read()\n",
    "originalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spun_text, df = paraphraser.parapherase(originalText, mask=0.1, range_replace=(1, 4), mark_replace=True, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ayn Rand (; born Alisa Zinovyevna Rosenbaum; Â\\xa0â€“[September] 6, 1982) was a Russian[—]American[ novelist] and[ educator]. She is known for her two best-selling novels, \"The Fountainhead\" and \"Atlas[ Sch]rugged\", and for developing a philosophical[ school] she named Objectivism. Educated in Russia, she moved to the United States[ by] 1926. She had a play produced on Broadway in 1935 and 1936[,] After[ writing] early novels that were initially unsuccessful, she[ gained] fame with her 1943 novel,[\"]The Fountainhead\". In 1957[,] Rand published her best-known work, the[ memoir] \"Atlas Shrugged\". Afterward, she turned to[ Non]-fiction to promote her philosophy[ by] publishing her own periodicals and releasing several collections of essays[ after] her death[ of] 1982.'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "spun_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            token_str     score    state\n",
       "index token                             \n",
       "96    4             .  0.999610  ignored\n",
       "      479          Ġ.  0.000186   looked\n",
       "      6             ,  0.000062   chosen\n",
       "      7586         ..  0.000053   looked\n",
       "31    12            -  0.964292  ignored\n",
       "...               ...       ...      ...\n",
       "82    30          Ġby  0.000150   chosen\n",
       "121   6             ,  0.999708  ignored\n",
       "      2156         Ġ,  0.000069   chosen\n",
       "      4             .  0.000048   looked\n",
       "      131           ;  0.000011   looked\n",
       "\n",
       "[68 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>token_str</th>\n      <th>score</th>\n      <th>state</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th>token</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">96</th>\n      <th>4</th>\n      <td>.</td>\n      <td>0.999610</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>479</th>\n      <td>Ġ.</td>\n      <td>0.000186</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>,</td>\n      <td>0.000062</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>7586</th>\n      <td>..</td>\n      <td>0.000053</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <th>12</th>\n      <td>-</td>\n      <td>0.964292</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <th>30</th>\n      <td>Ġby</td>\n      <td>0.000150</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">121</th>\n      <th>6</th>\n      <td>,</td>\n      <td>0.999708</td>\n      <td>ignored</td>\n    </tr>\n    <tr>\n      <th>2156</th>\n      <td>Ġ,</td>\n      <td>0.000069</td>\n      <td>chosen</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.</td>\n      <td>0.000048</td>\n      <td>looked</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>;</td>\n      <td>0.000011</td>\n      <td>looked</td>\n    </tr>\n  </tbody>\n</table>\n<p>68 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "### Iteratding over the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "ogpath = \"./../data/thesis/og\"\n",
    "sppath = \"./../data/thesis/sp\"\n",
    "ogfiles = [f for f in listdir(ogpath) if isfile(join(ogpath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1-ORIG-10.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ogfiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Data were collected and analysed using excels spreadsheet and presented them into graphs and tables. Where data was exported from the sources, they were analytically explained in detail, outlining the trends and figures in the data, such as either in the form of plotted graphs or tables with corresponding graphs. Table and graphs were relevant in the analysis of the findings, since it made clarification simple and less complex, in instances where graphs were used to support the table, the essentiality was to concretise the explanations further pictorially.'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "with open(join(ogpath, ogfiles[0])) as file:\n",
    "    originalText = file.read()\n",
    "originalText"
   ]
  },
  {
   "source": [
    "## Check for problems in the Data or incobatebilety"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import paraphrase_pipeline as ppipe\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines import FillMaskPipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "unmasker = FillMaskPipeline(model=model, tokenizer=tokenizer, use_fast=True, device=-1)\n",
    "# unmasker = pipeline('fill-mask', model='roberta-large')\n",
    "paraphraser = ppipe.ParaphrasePipeline(unmasker)\n",
    "tokenizer = paraphraser.tokenizer\n",
    "model = paraphraser.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Economic development in the past was seen regarding the planned alteration of the structure of production and employment so that agricultureÕs share of both production and employment declines while that of manufacturing and services increases (Todaro, 2013)Todaro claims that economic growth is a vital condition to improve the quality of life. Development was therefore defined as a rapid and sustained rise in real output per head and attendant shifts in the technological and economic characteristics of society. This conceptualisation gave priority to increased commodity output instead of the human beings involved in the production. Increases in the output of such industries were recorded as growth in the economy and for that matter development for the country (Kane, 2008).\\nEconomic development refers to a qualitative variation in what or how goods and services produced through shifts in resource use, workforce skills, technology, information, production methods, or financial arrangements. In other words, it is the growth of the countryÕs wealth that is to improve the well-being of the countryÕs inhabitants (Todaro, 2013). Economic improvement enhances the regional economyÕs capability to generate wealth for residents. It depends upon deployment of a regionÕs building blocks â\\x80\\x93 financial capital, labour, technological know-how, facilities, equipment, land, other physical resources, and public and private infrastructure (Sarpong, 2011). A regional economy can grow without changing if it directly produces more of the same goods and services in the same manner. For example, an increment in the population of an area will mean more income and more demand-driven growth even absent of qualitative changes in the economic development environment.\\nPublic officials continue economic advancement initiatives to impel job growth, increase income for residents, expand the tax base, raise property values, improve the quality of life, stabilise communities, reduce poverty, and even lower\\ncrime rates (Sarpong, 2011). In the drive to impact regional economies, policymakers invest public resources to economic development efforts. The anticipated payoffs are growth and development. However public resources may be misled or wasted if local and state governments engage in economic development purposes without a proper understanding of the opportunities and limits about public actions and regional growth. Local and state economic development efforts work best when they suit the role of public-sector action and build upon local potentials and strengths to improve the long-run prospects for economic growth and vitality (Sarpong, 2011).\\nThe Harrodâ\\x80\\x93Domar model represents a classical Keynesian model of economic growth. The development of economics is used to explain an economy's growth rate regarding the level of saving and productivity of capital. It recommends that there is no logical reason for an economy's balanced growth.\\nHarrodâ\\x80\\x93Domar model suggests there are three kinds of growth: real growth, warranted (guaranteed) growth and the natural rate of growth. The guaranteed growth rate is the rate of growth at which the economy does not develop indefinitely or go into recession. Actual growth is the correct rate increase in a country's GDP per year. Natural growth is the growth an economy needs to maintain sufficient employment. Example, if the labour force grows at 10% per year, then to maintain adequate employment, the economyÕs annual growth rate must be 10%.\\nThe theories of Harrod-Domar view savings to be a sufficient condition for development and growth. That is to say, if an economy saves, it will grow, and if it grows, it must develop. Aggregate savings are defined mainly by national income, so if income is low, savings will not be accumulated. Harrod-DomarÕs theory suggests that having a savings ratio of 0.15 â\\x80\\x93 0.2 would be adequate to provide the basis for growth. Maintaining this level of saving would sustain growth\\nHarrod-Domar models assume the following:\\n\\t1.\\tA full-employment level of existing income.\\n\\t2.\\tThere is no government restraint in the functioning of the economy.\\n\\t3.\\tThe model is built on the assumption of a â\\x80\\x9cclosed economy.â\\x80\\x9d In other words, government limitations on trade and the complications caused by international trade are ruled out.\\n\\t4.\\tThere are no lags in the adjustment of variables, i.e., the economic variables such as investment, savings, income, and expenditure adjust themselves entirely within the same period.\\n\\t5.\\tThe average propensity to save (APS) and marginal propensity to save (MPS) are equal to each other. APS = MPS or written in symbols,S/Y= â\\x88\\x86S/â\\x88\\x86Y\""
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "filename = r\"./../data/thesis/og/1-ORIG-18.txt\"\n",
    "with open(filename, 'r', encoding='latin-1') as file:\n",
    "    originalText = file.read()\n",
    "originalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [50265, 41376, 709, 11, 5, 375, 21, 450, 2624, 5, 1904, 39752, 9, 5, 3184, 9, 931, 8, 4042, 98, 14, 6300, 3849, 15722, 29, 458, 9, 258, 931, 8, 4042, 11081, 150, 14, 9, 3021, 8, 518, 3488, 36, 565, 1630, 5191, 6, 1014, 43, 565, 1630, 5191, 1449, 14, 776, 434, 16, 10, 4874, 1881, 7, 1477, 5, 1318, 9, 301, 4, 2717, 21, 3891, 6533, 25, 10, 6379, 8, 5232, 1430, 11, 588, 4195, 228, 471, 8, 22012, 10701, 11, 5, 9874, 8, 776, 12720, 9, 2313, 4, 152, 28647, 3258, 851, 3887, 7, 1130, 8497, 4195, 1386, 9, 5, 1050, 14766, 963, 11, 5, 931, 4, 43832, 11, 5, 4195, 9, 215, 4510, 58, 2673, 25, 434, 11, 5, 866, 8, 13, 14, 948, 709, 13, 5, 247, 36, 530, 1728, 6, 2266, 322, 50118, 41376, 709, 12859, 7, 10, 29981, 21875, 11, 99, 50, 141, 3057, 8, 518, 2622, 149, 10701, 11, 5799, 304, 6, 6862, 2417, 6, 806, 6, 335, 6, 931, 6448, 6, 50, 613, 7863, 4, 96, 97, 1617, 6, 24, 16, 5, 434, 9, 5, 247, 3849, 15722, 29, 4764, 14, 16, 7, 1477, 5, 157, 12, 9442, 9, 5, 247, 3849, 15722, 29, 24696, 36, 565, 1630, 5191, 6, 1014, 322, 4713, 3855, 27460, 5, 2174, 866, 3849, 15722, 29, 9388, 7, 5368, 4764, 13, 1196, 4, 85, 7971, 2115, 9737, 9, 10, 976, 3849, 15722, 29, 745, 5491, 952, 7258, 4056, 7471, 4056, 9085, 613, 812, 6, 6610, 6, 9874, 216, 12, 9178, 6, 2644, 6, 2104, 6, 1212, 6, 97, 2166, 1915, 6, 8, 285, 8, 940, 2112, 36, 104, 11230, 1657, 6, 1466, 322, 83, 2174, 866, 64, 1733, 396, 2992, 114, 24, 2024, 9108, 55, 9, 5, 276, 3057, 8, 518, 11, 5, 276, 4737, 4, 286, 1246, 6, 41, 30401, 11, 5, 1956, 9, 41, 443, 40, 1266, 55, 1425, 8, 55, 1077, 12, 9756, 434, 190, 11640, 9, 29981, 1022, 11, 5, 776, 709, 1737, 4, 50118, 22649, 503, 535, 776, 19026, 5287, 7, 4023, 523, 633, 434, 6, 712, 1425, 13, 1196, 6, 3003, 5, 629, 1542, 6, 1693, 1038, 3266, 6, 1477, 5, 1318, 9, 301, 6, 12964, 1496, 1822, 6, 1888, 5263, 6, 8, 190, 795, 50118, 18608, 1162, 36, 104, 11230, 1657, 6, 1466, 322, 96, 5, 1305, 7, 913, 2174, 6795, 6, 12875, 3754, 285, 1915, 7, 776, 709, 1170, 4, 20, 5291, 582, 10816, 32, 434, 8, 709, 4, 635, 285, 1915, 189, 28, 25022, 50, 14260, 114, 400, 8, 194, 3233, 4949, 11, 776, 709, 6216, 396, 10, 4692, 2969, 9, 5, 1616, 8, 4971, 59, 285, 2163, 8, 2174, 434, 4, 4004, 8, 194, 776, 709, 1170, 173, 275, 77, 51, 3235, 5, 774, 9, 285, 12, 18658, 814, 8, 1119, 2115, 400, 801, 29, 8, 13348, 7, 1477, 5, 251, 12, 2962, 5108, 13, 776, 434, 8, 35762, 36, 104, 11230, 1657, 6, 1466, 322, 50118, 133, 2482, 10774, 3695, 4056, 7471, 4056, 9085, 39914, 271, 1421, 3372, 10, 15855, 33897, 811, 1421, 9, 776, 434, 4, 20, 709, 9, 10492, 16, 341, 7, 3922, 41, 866, 18, 434, 731, 2624, 5, 672, 9, 6549, 8, 8106, 9, 812, 4, 85, 10827, 14, 89, 16, 117, 16437, 1219, 13, 41, 866, 18, 9320, 434, 4, 50118, 17488, 10774, 3695, 4056, 7471, 4056, 9085, 39914, 271, 1421, 3649, 89, 32, 130, 6134, 9, 434, 35, 588, 434, 6, 24939, 36, 5521, 19112, 10247, 43, 434, 8, 5, 1632, 731, 9, 434, 4, 20, 8045, 434, 731, 16, 5, 731, 9, 434, 23, 61, 5, 866, 473, 45, 2179, 17675, 50, 213, 88, 7306, 4, 30144, 434, 16, 5, 4577, 731, 712, 11, 10, 247, 18, 6250, 228, 76, 4, 7278, 434, 16, 5, 434, 41, 866, 782, 7, 3014, 7719, 4042, 4, 42516, 6, 114, 5, 6610, 1370, 11461, 23, 158, 207, 228, 76, 6, 172, 7, 3014, 9077, 4042, 6, 5, 866, 3849, 15722, 29, 1013, 434, 731, 531, 28, 158, 2153, 50118, 133, 14831, 9, 2482, 10774, 12, 39914, 271, 1217, 4522, 7, 28, 10, 7719, 1881, 13, 709, 8, 434, 4, 280, 16, 7, 224, 6, 114, 41, 866, 7552, 6, 24, 40, 1733, 6, 8, 114, 24, 11461, 6, 24, 531, 2179, 4, 14644, 41101, 4522, 32, 6533, 4412, 30, 632, 1425, 6, 98, 114, 1425, 16, 614, 6, 4522, 40, 45, 28, 15323, 4, 2482, 10774, 12, 39914, 271, 3849, 15722, 29, 6680, 3649, 14, 519, 10, 4522, 1750, 9, 321, 4, 996, 952, 7258, 4056, 7471, 4056, 9085, 321, 4, 176, 74, 28, 9077, 7, 694, 5, 1453, 13, 434, 4, 256, 12042, 8173, 42, 672, 9, 6549, 74, 9844, 434, 50118, 17488, 10774, 12, 39914, 271, 3092, 6876, 5, 511, 35, 50118, 50117, 134, 4, 50117, 250, 455, 12, 37011, 672, 9, 2210, 1425, 4, 50118, 50117, 176, 4, 50117, 970, 16, 117, 168, 20219, 11, 5, 13838, 9, 5, 866, 4, 50118, 50117, 246, 4, 50117, 133, 1421, 16, 1490, 15, 5, 15480, 9, 10, 952, 7258, 4056, 7471, 4056, 48, 25315, 866, 4, 3695, 4056, 7471, 4056, 46, 96, 97, 1617, 6, 168, 11948, 15, 721, 8, 5, 12385, 1726, 30, 758, 721, 32, 3447, 66, 4, 50118, 50117, 306, 4, 50117, 970, 32, 117, 784, 8299, 11, 5, 13380, 9, 25083, 6, 939, 4, 242, 482, 5, 776, 25083, 215, 25, 915, 6, 4522, 6, 1425, 6, 8, 12187, 9160, 1235, 4378, 624, 5, 276, 675, 4, 50118, 50117, 245, 4, 50117, 133, 674, 36186, 7, 1871, 36, 31959, 43, 8, 14612, 36186, 7, 1871, 36, 448, 3888, 43, 32, 3871, 7, 349, 97, 4, 1480, 104, 5457, 256, 3888, 50, 1982, 11, 19830, 6, 104, 73, 975, 5214, 952, 7258, 4056, 23133, 4056, 27819, 104, 73, 3695, 4056, 23133, 4056, 27819, 975, 50266], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tokens = tokenizer(originalText)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "968"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_input = tokenizer(originalText,  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-99344ea9d35d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencode_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m         )\n\u001b[0;32m    907\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         embedding_output = self.embeddings(\n\u001b[1;32m--> 684\u001b[1;33m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         )\n\u001b[0;32m    686\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "output_tensor = model(**encode_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLongText = ' '.join(511*['you']) # 966 to long, 510 is ok, 511 is to long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "len(tokenizer(testLongText)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLong_input = tokenizer(testLongText,  return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 513])\ntorch.Size([1, 513])\n"
     ]
    }
   ],
   "source": [
    "print(testLong_input['input_ids'].size())\n",
    "print(testLong_input['attention_mask'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-300462930b06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtestLong_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m         )\n\u001b[0;32m    907\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         embedding_output = self.embeddings(\n\u001b[1;32m--> 684\u001b[1;33m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         )\n\u001b[0;32m    686\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1722\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model(**testLong_input)"
   ]
  },
  {
   "source": [
    "### Input size is bounded\n",
    "\n",
    "The input dimension of the model is 1024 and with out the attentionmast 512 and with out the added\n",
    "start and end tokens 510."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import paraphrase_pipeline as ppipe\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines import FillMaskPipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-large\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "unmasker = FillMaskPipeline(model=model, tokenizer=tokenizer, use_fast=True, device=-1)\n",
    "# unmasker = pipeline('fill-mask', model='roberta-large')\n",
    "paraphraser = ppipe.ParaphrasePipeline(unmasker)\n",
    "tokenizer = paraphraser.tokenizer\n",
    "model = paraphraser.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"./../data/thesis/og/1-ORIG-18.txt\"\n",
    "with open(filename, 'r', encoding='latin-1') as file:\n",
    "    originalText = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "shift: 456\n",
      "masked_index_in tensor([[759]]), masked_index_out: tensor([[303]])\n",
      "shift: 159\n",
      "masked_index_in tensor([[414]]), masked_index_out: tensor([[255]])\n",
      "shift: 32\n",
      "masked_index_in tensor([[287]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[894]]), masked_index_out: tensor([[438]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[840]]), masked_index_out: tensor([[384]])\n",
      "shift: 102\n",
      "masked_index_in tensor([[357]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[807]]), masked_index_out: tensor([[351]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[816]]), masked_index_out: tensor([[360]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[37]]), masked_index_out: tensor([[37]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[3]]), masked_index_out: tensor([[3]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[742]]), masked_index_out: tensor([[286]])\n",
      "shift: 454\n",
      "masked_index_in tensor([[709]]), masked_index_out: tensor([[255]])\n",
      "shift: 86\n",
      "masked_index_in tensor([[341]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[711]]), masked_index_out: tensor([[255]])\n",
      "shift: 151\n",
      "masked_index_in tensor([[406]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[866]]), masked_index_out: tensor([[410]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[225]]), masked_index_out: tensor([[225]])\n",
      "shift: 447\n",
      "masked_index_in tensor([[702]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[191]]), masked_index_out: tensor([[191]])\n",
      "shift: 111\n",
      "masked_index_in tensor([[366]]), masked_index_out: tensor([[255]])\n",
      "shift: 344\n",
      "masked_index_in tensor([[599]]), masked_index_out: tensor([[255]])\n",
      "shift: 222\n",
      "masked_index_in tensor([[477]]), masked_index_out: tensor([[255]])\n",
      "shift: 333\n",
      "masked_index_in tensor([[588]]), masked_index_out: tensor([[255]])\n",
      "shift: 97\n",
      "masked_index_in tensor([[352]]), masked_index_out: tensor([[255]])\n",
      "shift: 450\n",
      "masked_index_in tensor([[705]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[839]]), masked_index_out: tensor([[383]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[180]]), masked_index_out: tensor([[180]])\n",
      "shift: 51\n",
      "masked_index_in tensor([[306]]), masked_index_out: tensor([[255]])\n",
      "shift: 281\n",
      "masked_index_in tensor([[536]]), masked_index_out: tensor([[255]])\n",
      "shift: 394\n",
      "masked_index_in tensor([[649]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[747]]), masked_index_out: tensor([[291]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[718]]), masked_index_out: tensor([[262]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[163]]), masked_index_out: tensor([[163]])\n",
      "shift: 408\n",
      "masked_index_in tensor([[663]]), masked_index_out: tensor([[255]])\n",
      "shift: 284\n",
      "masked_index_in tensor([[539]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[200]]), masked_index_out: tensor([[200]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[930]]), masked_index_out: tensor([[474]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[712]]), masked_index_out: tensor([[256]])\n",
      "shift: 352\n",
      "masked_index_in tensor([[607]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[925]]), masked_index_out: tensor([[469]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[795]]), masked_index_out: tensor([[339]])\n",
      "shift: 128\n",
      "masked_index_in tensor([[383]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[886]]), masked_index_out: tensor([[430]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[83]]), masked_index_out: tensor([[83]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[32]]), masked_index_out: tensor([[32]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[177]]), masked_index_out: tensor([[177]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[69]]), masked_index_out: tensor([[69]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[66]]), masked_index_out: tensor([[66]])\n",
      "shift: 65\n",
      "masked_index_in tensor([[320]]), masked_index_out: tensor([[255]])\n",
      "shift: 276\n",
      "masked_index_in tensor([[531]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[867]]), masked_index_out: tensor([[411]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[124]]), masked_index_out: tensor([[124]])\n",
      "shift: 311\n",
      "masked_index_in tensor([[566]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[754]]), masked_index_out: tensor([[298]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[756]]), masked_index_out: tensor([[300]])\n",
      "shift: 286\n",
      "masked_index_in tensor([[541]]), masked_index_out: tensor([[255]])\n",
      "shift: 262\n",
      "masked_index_in tensor([[517]]), masked_index_out: tensor([[255]])\n",
      "shift: 349\n",
      "masked_index_in tensor([[604]]), masked_index_out: tensor([[255]])\n",
      "shift: 361\n",
      "masked_index_in tensor([[616]]), masked_index_out: tensor([[255]])\n",
      "shift: 275\n",
      "masked_index_in tensor([[530]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[728]]), masked_index_out: tensor([[272]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[58]]), masked_index_out: tensor([[58]])\n",
      "shift: 129\n",
      "masked_index_in tensor([[384]]), masked_index_out: tensor([[255]])\n",
      "shift: 431\n",
      "masked_index_in tensor([[686]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[734]]), masked_index_out: tensor([[278]])\n",
      "shift: 390\n",
      "masked_index_in tensor([[645]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[846]]), masked_index_out: tensor([[390]])\n",
      "shift: 401\n",
      "masked_index_in tensor([[656]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[26]]), masked_index_out: tensor([[26]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[250]]), masked_index_out: tensor([[250]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[860]]), masked_index_out: tensor([[404]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[721]]), masked_index_out: tensor([[265]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[95]]), masked_index_out: tensor([[95]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[18]]), masked_index_out: tensor([[18]])\n",
      "shift: 8\n",
      "masked_index_in tensor([[263]]), masked_index_out: tensor([[255]])\n",
      "shift: 47\n",
      "masked_index_in tensor([[302]]), masked_index_out: tensor([[255]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[938]]), masked_index_out: tensor([[482]])\n",
      "shift: 231\n",
      "masked_index_in tensor([[486]]), masked_index_out: tensor([[255]])\n",
      "shift: 446\n",
      "masked_index_in tensor([[701]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[13]]), masked_index_out: tensor([[13]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[155]]), masked_index_out: tensor([[155]])\n",
      "shift: 15\n",
      "masked_index_in tensor([[270]]), masked_index_out: tensor([[255]])\n",
      "shift: 307\n",
      "masked_index_in tensor([[562]]), masked_index_out: tensor([[255]])\n",
      "shift: 52\n",
      "masked_index_in tensor([[307]]), masked_index_out: tensor([[255]])\n",
      "shift: 197\n",
      "masked_index_in tensor([[452]]), masked_index_out: tensor([[255]])\n",
      "shift: 33\n",
      "masked_index_in tensor([[288]]), masked_index_out: tensor([[255]])\n",
      "shift: 324\n",
      "masked_index_in tensor([[579]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[223]]), masked_index_out: tensor([[223]])\n",
      "shift: 456\n",
      "masked_index_in tensor([[783]]), masked_index_out: tensor([[327]])\n",
      "shift: 232\n",
      "masked_index_in tensor([[487]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[248]]), masked_index_out: tensor([[248]])\n",
      "shift: 110\n",
      "masked_index_in tensor([[365]]), masked_index_out: tensor([[255]])\n",
      "shift: 61\n",
      "masked_index_in tensor([[316]]), masked_index_out: tensor([[255]])\n",
      "shift: 388\n",
      "masked_index_in tensor([[643]]), masked_index_out: tensor([[255]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[230]]), masked_index_out: tensor([[230]])\n",
      "shift: 0\n",
      "masked_index_in tensor([[189]]), masked_index_out: tensor([[189]])\n"
     ]
    }
   ],
   "source": [
    "spun_text, df = paraphraser.parapherase(originalText, mask=0.1, range_replace=(1, 4), mark_replace=True, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Economic development[ during] the past was seen regarding the planned alteration of[ a] structure of production and[ production] so that agricultureÕs share[ in] both production and employment declines[ while] that of manufacturing and[ consumption] increases (Todaro, 2013)Todaro claims that economic growth is a vital condition to[ enhancing] the quality of life. Development was[ previously] defined as[ any] rapid and sustained rise in real output per head and attendant shifts in[ underlying] technological and economic characteristics of society. This conceptualisation gave[ prominence] to increased commodity output instead of the human beings involved in the production. Increases in the output of such industries were recorded as growth in the economy[ not] for that matter development for the country (Kane, 2008).\\nEconomic development refers to a qualitative variation in what or how goods and services produced through[ shifts] in resource use, workforce skills,[ knowledge], information, production methods, or financial arrangements. In other words[,] it is[ a] growth of the countryÕs wealth[ which] is[ intended] improve the well-being of the country[�]�s inhabitants (Todaro, 2013). Economic improvement enhances the regional economyÕs capability to generate[ incomes] for[ society]. It depends upon[ some] of a regionÕs building blocks â\\x80\\x93 financial capital,[ people],[ technological] know-how, facilities, equipment, land, other physical[ properties], and public and private infrastructure[.(]Sarpong, 2011). A regional economy can grow without changing if it directly[ producing][ some] of the same goods and services in the same manner. For example[:] an increment in[ a][ wealth] of an area will mean more income and[ hence] demand-driven[ growth] even absent of qualitative changes in the economic development environment.\\nPublic officials continue economic advancement initiatives to imp[ on] job growth, increase income for residents, expand the[ resource] base, raise property[ taxes], improve the quality of life,[ modern][ised] communities, reduce poverty, and even lower\\ncrime rates (Sarpong,[ 2010][ ).] In the drive to impact regional economies, policymakers invest public resources to economic development efforts. The anticipated payoffs[ include] growth and development. However public resources[ may] be misled or wasted if local and state governments engage in economic development purposes without a proper understanding of the opportunities and limits about public actions and regional growth. Local and state economic development efforts work[ well] when they suit the role of public-sector action and build upon local potentials and strengths to improve the long-run[ potential] for economic growth and vitality (Sarp[ung][,,] 2011).\\nThe Harrodâ\\x80\\x93Domar model represents a classical Keynesian model of economic growth. The development of economics is[ supposed] to explain an economy's growth rate regarding the level of saving[ or][ use] of capital. It[ states] that there[ exists] no[ one] reason for an economy's balanced growth.\\nHarrodâ\\x80\\x93Domar model suggests[ their] are three kinds[ for] growth: real growth, warranted (guaranteed) growth[ or] the natural rate of growth. The guaranteed[ Growth] rate is the rate of growth at which the economy[ does] not develop indefinitely or[ go] into recession[;] Actual growth is the correct rate increase in[ each] country's GDP per year. Natural growth is the growth an economy needs to maintain sufficient employment. Example, if the labour force grows[ at] 10[ %] per year,[ but] to maintain adequate employment, the[ country]Õs annual growth rate[ would] be 10%.\\nThe theories of Harrod-Domar view savings to be a sufficient condition for development and[ expansion]. That is to say, if an economy saves, it will grow[;][ or] if it[ spends], it must[ invest].[Agg][,] savings are defined mainly by[ current] income,[ so] if income is low, savings[ would] not be accumulated. Har[od]-DomarÕs theory[ assumes] that having a savings[ coefficient] of 0.15 â[�]�[ �]� 0[·]2 would be adequate to provide the basis for growth. Maintaining this level of saving would sustain growth\\nHar[row]-Domar models assume the following:\\n\\t1[..]\\tA full-employment level of existing income.\\n[#]2.\\tThere is no government restraint[ upon] the functioning of the economy.\\n\\t3.\\tThe model is built on the assumption of a â[�][�]\\x9cclosed economy.[€]\\x80\\x9d In other words, government limitations on trade and[ other] complications caused by international trade[ is][ kept] out.\\n\\t4.\\tThere are no lags in the adjustment of variables,[i].e., the economic variables such[ an] investment, savings, income, and expenditure adjust themselves entirely within the same period.\\n\\t5.\\tThe average propensity to save (APS) and[ median] propensity to save ([m]PS) are equal to each other[ as] APS = MPS or written in symbols,S/Y= â\\x88\\x86S/â\\x88\\x86Y\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spun_text"
   ]
  },
  {
   "source": [
    "### Getting Model inputsize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Embedding(50269, 1024)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "model.get_output_embeddings().in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}